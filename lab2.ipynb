{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8be019a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aa45ae0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life', 'use', 'make', 'paragraph', 'group', 'long', 'reading', 'easier', '.', 'upon', 'Without', 'Soil', 'A', 'erosion', 'ideas', 'also', 'our', 'well-being', 'organize', 'closely', 'be', 'text', ',', 'air', 'ability', 'nature', 'in', 'would', 'sentences', 'We', 'sets', 'homes', 'trees', 'into', 'food', ':', 'related', 'provides', 'paragraphs', 'groups', 'enjoyment', 'we', 'Example', 'of', 'grow', 'difficult', 'become', 'a', 'us', 'the', 'crops', 'texts', 'impairs', 'grouping', 'Human', 'break', 'is', 'depends', 'with', 'organizing', 'logical', 'Diverse', 'animal', 'endangers', 'sequences', 'sustain', 'clean', 'comprehension', 'weak', 'unpleasant', 'shorter', 'ill', 'and', 'to'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "str=\"A paragraph is a group of sentences with closely related ideas. We use paragraphs to break long texts into shorter groups of ideas, to make reading and comprehension easier. We also use paragraphs to organize the ideas in a text, grouping related ideas into sets, and organizing sets of ideas into logical sequences. Example: Human well-being depends upon trees. Without clean air we become ill and weak. Diverse animal life provides us with food and enjoyment of nature. Soil erosion impairs our ability to grow crops and endangers our homes. Without trees, life would be unpleasant and difficult to sustain\"\n",
    "words = set(word_tokenize(str))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7c084064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Soil erosion impairs our ability to grow crops and endangers our homes.', 'Example: Human well-being depends upon trees.', 'We also use paragraphs to organize the ideas in a text, grouping related ideas into sets, and organizing sets of ideas into logical sequences.', 'Without trees, life would be unpleasant and difficult to sustain', 'We use paragraphs to break long texts into shorter groups of ideas, to make reading and comprehension easier.', 'A paragraph is a group of sentences with closely related ideas.', 'Diverse animal life provides us with food and enjoyment of nature.', 'Without clean air we become ill and weak.'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences=set (sent_tokenize(str))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "af407634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ded1fd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'use', 'make', 'paragraph', 'group', 'long', 'reading', 'easier', '.', 'upon', 'Without', 'Soil', 'erosion', 'ideas', 'also', 'well-being', 'organize', 'closely', 'text', ',', 'air', 'ability', 'nature', 'would', 'sentences', 'sets', 'homes', 'trees', 'food', ':', 'related', 'provides', 'paragraphs', 'groups', 'enjoyment', 'Example', 'grow', 'difficult', 'become', 'us', 'crops', 'texts', 'impairs', 'grouping', 'Human', 'break', 'depends', 'organizing', 'logical', 'Diverse', 'animal', 'endangers', 'sequences', 'sustain', 'clean', 'comprehension', 'weak', 'unpleasant', 'shorter', 'ill']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words= []\n",
    "for i in words:\n",
    "    if i.lower() not in (stop_words):\n",
    "        filtered_words.append(i)\n",
    "        \n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e976ef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'use', 'make', 'paragraph', 'group', 'long', 'reading', 'easier', '.', 'upon', 'Without', 'Soil', 'A', 'erosion', 'ideas', 'also', 'our', 'well-being', 'organize', 'closely', 'be', 'text', ',', 'air', 'ability', 'nature', 'in', 'would', 'sentences', 'We', 'sets', 'homes', 'trees', 'into', 'food', ':', 'related', 'provides', 'paragraphs', 'groups', 'enjoyment', 'we', 'Example', 'of', 'grow', 'difficult', 'become', 'a', 'us', 'the', 'crops', 'texts', 'impairs', 'grouping', 'Human', 'break', 'is', 'depends', 'with', 'organizing', 'logical', 'Diverse', 'animal', 'endangers', 'sequences', 'sustain', 'clean', 'comprehension', 'weak', 'unpleasant', 'shorter', 'ill', 'and', 'to']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball= SnowballStemmer(language= 'english')\n",
    "snowballstrem=[]\n",
    "for word in words:\n",
    "    snowball.stem(word)\n",
    "    snowballstrem.append(word)\n",
    "print(snowballstrem)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d717512f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'use', 'make', 'paragraph', 'group', 'long', 'read', 'easier', '.', 'upon', 'without', 'soil', 'a', 'eros', 'idea', 'also', 'our', 'well-b', 'organ', 'close', 'be', 'text', ',', 'air', 'abil', 'natur', 'in', 'would', 'sentenc', 'we', 'set', 'home', 'tree', 'into', 'food', ':', 'relat', 'provid', 'paragraph', 'group', 'enjoy', 'we', 'exampl', 'of', 'grow', 'difficult', 'becom', 'a', 'us', 'the', 'crop', 'text', 'impair', 'group', 'human', 'break', 'is', 'depend', 'with', 'organ', 'logic', 'divers', 'anim', 'endang', 'sequenc', 'sustain', 'clean', 'comprehens', 'weak', 'unpleas', 'shorter', 'ill', 'and', 'to']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "porterstem=[porter.stem(word) for word in words]\n",
    "print (porterstem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6c085ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lif', 'us', 'mak', 'paragraph', 'group', 'long', 'read', 'easy', '.', 'upon', 'without', 'soil', 'a', 'erod', 'idea', 'also', 'our', 'well-being', 'org', 'clos', 'be', 'text', ',', 'air', 'abl', 'nat', 'in', 'would', 'sent', 'we', 'set', 'hom', 'tre', 'into', 'food', ':', 'rel', 'provid', 'paragraph', 'group', 'enjoy', 'we', 'exampl', 'of', 'grow', 'difficult', 'becom', 'a', 'us', 'the', 'crop', 'text', 'impair', 'group', 'hum', 'break', 'is', 'depend', 'with', 'org', 'log', 'divers', 'anim', 'endang', 'sequ', 'sustain', 'cle', 'comprehend', 'weak', 'unpleas', 'short', 'il', 'and', 'to']\n"
     ]
    }
   ],
   "source": [
    "from nltk import LancasterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "lancasterstem=[lancaster.stem(word) for word in words]\n",
    "print (lancasterstem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e78800ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lif', 'u', 'mak', 'paragraph', 'group', 'long', 'rad', 'air', '.', 'upon', 'Without', 'Soil', 'A', 'roion', 'ida', 'alo', 'our', 'wll-b', 'organiz', 'cloly', 'b', 'txt', ',', 'air', 'ability', 'natur', 'in', 'would', 'ntnc', 'W', 't', 'hom', 'tr', 'into', 'food', ':', 'rlatd', 'provid', 'paragraph', 'group', 'njoymnt', 'w', 'Exampl', 'of', 'grow', 'difficult', 'bcom', 'a', 'u', 'th', 'crop', 'txt', 'impair', 'group', 'Human', 'brak', 'i', 'dpnd', 'with', 'organiz', 'logical', 'Divr', 'animal', 'ndangr', 'qunc', 'utain', 'clan', 'comprhnion', 'wak', 'unplaant', 'hortr', 'ill', 'and', 'to']\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpStemmer\n",
    "regexp= RegexpStemmer('ing|s|e|able|ed')\n",
    "regexpstem=[regexp.stem(word) for word in words]\n",
    "print (regexpstem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "940105e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'use', 'make', 'paragraph', 'group', 'long', 'reading', 'easier', '.', 'upon', 'Without', 'Soil', 'A', 'erosion', 'idea', 'also', 'our', 'well-being', 'organize', 'closely', 'be', 'text', ',', 'air', 'ability', 'nature', 'in', 'would', 'sentence', 'We', 'set', 'home', 'tree', 'into', 'food', ':', 'related', 'provides', 'paragraph', 'group', 'enjoyment', 'we', 'Example', 'of', 'grow', 'difficult', 'become', 'a', 'u', 'the', 'crop', 'text', 'impairs', 'grouping', 'Human', 'break', 'is', 'depends', 'with', 'organizing', 'logical', 'Diverse', 'animal', 'endangers', 'sequence', 'sustain', 'clean', 'comprehension', 'weak', 'unpleasant', 'shorter', 'ill', 'and', 'to']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer =WordNetLemmatizer()\n",
    "lemmatizied_words=[lemmatizer.lemmatize(word) for word in words]\n",
    "print (lemmatizied_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5ec459f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      Snowball Stemmer    Lancaster Stemmer             Regexp Stemmer                          WordNetLemmatizer                                 \n",
      "friend              friend              friend              friend                        frind                                   friend                                            \n",
      "friendship          friendship          friendship          friend                        frindhip                                friendship                                        \n",
      "friends             friend              friend              friend                        frind                                   friend                                            \n",
      "friendships         friendship          friendship          friend                        frindhip                                friendship                                        \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer, WordNetLemmatizer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(language='english')\n",
    "regexp = RegexpStemmer('ing|s|e|able|ed', min=4)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\"]\n",
    "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}{5:50}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer','WordNetLemmatizer'))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}{5:50}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word),lemmatizer.lemmatize(word)))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5087b79d",
   "metadata": {},
   "source": [
    "Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1290a034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'models', 'are', 'transforming', 'the', 'world', 'rapidly', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "string = \"NLP models are transforming the world rapidly!!\"\n",
    "word_tokens = word_tokenize(string)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6ca09f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'models', 'transforming', 'world', 'rapidly', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_new = set(stopwords.words(\"english\"))\n",
    "new_filtered_text= [word for word in word_tokens if word.lower() not in stop_words_new]\n",
    "print(new_filtered_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3b616986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nlp', 'model', 'transform', 'world', 'rapidli', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stemmed__string =[ps.stem(word) for word in new_filtered_text]\n",
    "print(stemmed__string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d8c77ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'model', 'transforming', 'world', 'rapidly', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_string = [lemmatizer.lemmatize(word) for word in new_filtered_text]\n",
    "print(lemmatized_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5510e",
   "metadata": {},
   "source": [
    "assignment: lab 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "44441fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'medical', 'text', 'corpus', 'is', 'a', 'large', 'and', 'organized', 'collection', 'of', 'healthcare-related', 'textual', 'data', 'used', 'for', 'medical', 'research', ',', 'education', ',', 'and', 'natural', 'language', 'processing', 'applications', '.', 'It', 'includes', 'data', 'such', 'as', 'clinical', 'notes', ',', 'electronic', 'health', 'records', ',', 'medical', 'reports', ',', 'research', 'articles', ',', 'and', 'treatment', 'guidelines', '.', 'Medical', 'text', 'corpora', 'help', 'computers', 'understand', 'complex', 'medical', 'language', ',', 'terminology', ',', 'and', 'abbreviations', ',', 'enabling', 'tasks', 'like', 'disease', 'detection', ',', 'clinical', 'decision', 'support', ',', 'medical', 'chatbots', ',', 'and', 'information', 'extraction', '.', 'These', 'corpora', 'are', 'often', 'anonymized', 'to', 'protect', 'patient', 'privacy', 'and', 'are', 'essential', 'for', 'developing', 'accurate', 'and', 'reliable', 'healthcare', 'AI', 'systems', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text= \"A medical text corpus is a large and organized collection of healthcare-related textual data used for medical research, education, and natural language processing applications. It includes data such as clinical notes, electronic health records, medical reports, research articles, and treatment guidelines. Medical text corpora help computers understand complex medical language, terminology, and abbreviations, enabling tasks like disease detection, clinical decision support, medical chatbots, and information extraction. These corpora are often anonymized to protect patient privacy and are essential for developing accurate and reliable healthcare AI systems.\"\n",
    "tokens = []\n",
    "for word in word_tokenize(text):\n",
    "    tokens.append(word)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d071a371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['medical', 'text', 'corpus', 'large', 'organized', 'collection', 'healthcare-related', 'textual', 'data', 'used', 'medical', 'research', ',', 'education', ',', 'natural', 'language', 'processing', 'applications', '.', 'includes', 'data', 'clinical', 'notes', ',', 'electronic', 'health', 'records', ',', 'medical', 'reports', ',', 'research', 'articles', ',', 'treatment', 'guidelines', '.', 'Medical', 'text', 'corpora', 'help', 'computers', 'understand', 'complex', 'medical', 'language', ',', 'terminology', ',', 'abbreviations', ',', 'enabling', 'tasks', 'like', 'disease', 'detection', ',', 'clinical', 'decision', 'support', ',', 'medical', 'chatbots', ',', 'information', 'extraction', '.', 'corpora', 'often', 'anonymized', 'protect', 'patient', 'privacy', 'essential', 'developing', 'accurate', 'reliable', 'healthcare', 'AI', 'systems', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_text=[]\n",
    "for word in tokens:\n",
    "    if word.lower() not in stop_words:\n",
    "        filtered_text.append(word)\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e7f51d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['medic', 'text', 'corpu', 'larg', 'organ', 'collect', 'healthcare-rel', 'textual', 'data', 'use', 'medic', 'research', ',', 'educ', ',', 'natur', 'languag', 'process', 'applic', '.', 'includ', 'data', 'clinic', 'note', ',', 'electron', 'health', 'record', ',', 'medic', 'report', ',', 'research', 'articl', ',', 'treatment', 'guidelin', '.', 'medic', 'text', 'corpora', 'help', 'comput', 'understand', 'complex', 'medic', 'languag', ',', 'terminolog', ',', 'abbrevi', ',', 'enabl', 'task', 'like', 'diseas', 'detect', ',', 'clinic', 'decis', 'support', ',', 'medic', 'chatbot', ',', 'inform', 'extract', '.', 'corpora', 'often', 'anonym', 'protect', 'patient', 'privaci', 'essenti', 'develop', 'accur', 'reliabl', 'healthcar', 'ai', 'system', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed_text=[porter.stem(word)for word in filtered_text]\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "13d5c7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['medical', 'text', 'corpus', 'large', 'organized', 'collection', 'healthcare-related', 'textual', 'data', 'used', 'medical', 'research', ',', 'education', ',', 'natural', 'language', 'processing', 'application', '.', 'includes', 'data', 'clinical', 'note', ',', 'electronic', 'health', 'record', ',', 'medical', 'report', ',', 'research', 'article', ',', 'treatment', 'guideline', '.', 'Medical', 'text', 'corpus', 'help', 'computer', 'understand', 'complex', 'medical', 'language', ',', 'terminology', ',', 'abbreviation', ',', 'enabling', 'task', 'like', 'disease', 'detection', ',', 'clinical', 'decision', 'support', ',', 'medical', 'chatbots', ',', 'information', 'extraction', '.', 'corpus', 'often', 'anonymized', 'protect', 'patient', 'privacy', 'essential', 'developing', 'accurate', 'reliable', 'healthcare', 'AI', 'system', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatize =WordNetLemmatizer()\n",
    "lemmatized_text=[lemmatize.lemmatize(word) for word in filtered_text]\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b990f57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare between stemmed and lemmatized text\n",
      "stemmed text..............lemmatized text\n",
      "medic                      medical\n",
      "text                      text\n",
      "corpu                      corpus\n",
      "larg                      large\n",
      "organ                      organized\n",
      "collect                      collection\n",
      "healthcare-rel                      healthcare-related\n",
      "textual                      textual\n",
      "data                      data\n",
      "use                      used\n",
      "medic                      medical\n",
      "research                      research\n",
      ",                      ,\n",
      "educ                      education\n",
      ",                      ,\n",
      "natur                      natural\n",
      "languag                      language\n",
      "process                      processing\n",
      "applic                      application\n",
      ".                      .\n",
      "includ                      includes\n",
      "data                      data\n",
      "clinic                      clinical\n",
      "note                      note\n",
      ",                      ,\n",
      "electron                      electronic\n",
      "health                      health\n",
      "record                      record\n",
      ",                      ,\n",
      "medic                      medical\n",
      "report                      report\n",
      ",                      ,\n",
      "research                      research\n",
      "articl                      article\n",
      ",                      ,\n",
      "treatment                      treatment\n",
      "guidelin                      guideline\n",
      ".                      .\n",
      "medic                      Medical\n",
      "text                      text\n",
      "corpora                      corpus\n",
      "help                      help\n",
      "comput                      computer\n",
      "understand                      understand\n",
      "complex                      complex\n",
      "medic                      medical\n",
      "languag                      language\n",
      ",                      ,\n",
      "terminolog                      terminology\n",
      ",                      ,\n",
      "abbrevi                      abbreviation\n",
      ",                      ,\n",
      "enabl                      enabling\n",
      "task                      task\n",
      "like                      like\n",
      "diseas                      disease\n",
      "detect                      detection\n",
      ",                      ,\n",
      "clinic                      clinical\n",
      "decis                      decision\n",
      "support                      support\n",
      ",                      ,\n",
      "medic                      medical\n",
      "chatbot                      chatbots\n",
      ",                      ,\n",
      "inform                      information\n",
      "extract                      extraction\n",
      ".                      .\n",
      "corpora                      corpus\n",
      "often                      often\n",
      "anonym                      anonymized\n",
      "protect                      protect\n",
      "patient                      patient\n",
      "privaci                      privacy\n",
      "essenti                      essential\n",
      "develop                      developing\n",
      "accur                      accurate\n",
      "reliabl                      reliable\n",
      "healthcar                      healthcare\n",
      "ai                      AI\n",
      "system                      system\n",
      ".                      .\n",
      "............................................................................\n",
      "since in medical records actual words are required to understand the context better lemmatization is preferred over stemming\n"
     ]
    }
   ],
   "source": [
    "print(\"compare between stemmed and lemmatized text\")\n",
    "print (\"stemmed text..............lemmatized text\")\n",
    "for i in range (len (stemmed_text)):\n",
    "    print(f\"{stemmed_text[i]}                      {lemmatized_text[i]}\")\n",
    "print(\"............................................................................\")\n",
    "print(\"since in medical records actual words are required to understand the context better lemmatization is preferred over stemming\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
